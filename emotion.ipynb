{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laume/nlp_emotions/blob/master/emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1WY7c3J84Du",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Sentiment Analysis: Emotion in Text\n",
        "\n",
        "In a variation on the popular task of sentiment analysis, this dataset contains labels for the emotional content (such as happiness, sadness, and anger) of texts. Hundreds to thousands of examples across 13 labels.\n",
        "\n",
        "https://www.figure-eight.com/data-for-everyone/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7db2xRwlwhO",
        "colab_type": "text"
      },
      "source": [
        "## Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwe5PW_3YC38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "39429f82-1a88-40cc-d8b3-dc9c34af4650"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNiLpDaMi-1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = 'data/emotions'\n",
        "SETUP = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ymiU_4j5PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjFl1pWQj-E-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isdir(DATA_DIR):\n",
        "    SETUP = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM6vjOT3Xp22",
        "colab_type": "text"
      },
      "source": [
        "### Install libraries and perform setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U09eDyxV1d7b",
        "colab_type": "code",
        "outputId": "9446fe79-02d5-41bf-bfa5-3535611c79a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "source": [
        "if SETUP:\n",
        "    !pip install --upgrade --quiet dlai\n",
        "    !pip install -q -U toai==0.2.*\n",
        "    !pip install -q -U nb_black\n",
        "    !pip install -q -U tensorflow-datasets\n",
        "    !pip install -q -U --no-deps tensorflow-addons~=0.6\n",
        "    !pip install -q -U tensorflow_hub\n",
        "    !pip install -q -U git+https://github.com/huggingface/transformers\n",
        "    print(__import__(\"toai\").__version__)\n",
        "    print(f'dlai version: {__import__(\"dlai\").__version__}, tf version: {__import__(\"tensorflow\").__version__}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 380.8MB 43kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 45.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 49.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.1MB/s \n",
            "\u001b[?25h  Building wheel for dlai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 153kB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 35.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25h  Building wheel for fastparquet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 36.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 737kB 50.0MB/s \n",
            "\u001b[?25h  Building wheel for nb-black (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathspec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 860kB 49.3MB/s \n",
            "\u001b[?25h  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "0.2.7\n",
            "dlai version: 0.0.16, tf version: 2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bKTqVqOzcTQ",
        "colab_type": "code",
        "outputId": "48ccbcd6-a495-41bc-f4a0-fc4e3261101b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import dlai\n",
        "from dlai.imports import *\n",
        "from toai.imports import *\n",
        "from toai.data import DataBundle, DataParams, DataContainer\n",
        "from toai.metrics import sparse_top_2_categorical_accuracy\n",
        "from toai.utils import save_file, load_file\n",
        "from toai.models import save_keras_model, load_keras_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/toai/imports.py:70: UserWarning: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "  warnings.warn(str(error))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdZZ8vaUXMSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eaii262EH0ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGjyOim4YfVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SETUP:\n",
        "    DATA_DIR = Path(DATA_DIR)\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    !cp '/content/gdrive/My Drive/text_emotion.csv' {DATA_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v3e3dZVGgoF",
        "colab_type": "code",
        "outputId": "5219bd56-739c-46d4-cb85-651b77e768d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "!ls {DATA_DIR}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_emotion.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYVgq63IRgid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(DATA_DIR/'text_emotion.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ext2sLmdbRiW",
        "colab_type": "code",
        "outputId": "44ad2cfb-6745-444f-ebc1-6fc82130ed99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "df.head().T"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>1956967666</td>\n",
              "      <td>1956967696</td>\n",
              "      <td>1956967789</td>\n",
              "      <td>1956968416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <td>empty</td>\n",
              "      <td>sadness</td>\n",
              "      <td>sadness</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>author</th>\n",
              "      <td>xoshayzers</td>\n",
              "      <td>wannamama</td>\n",
              "      <td>coolfunky</td>\n",
              "      <td>czareaquino</td>\n",
              "      <td>xkilljoyx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>content</th>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           0  ...                                                  4\n",
              "tweet_id                                          1956967341  ...                                         1956968416\n",
              "sentiment                                              empty  ...                                            neutral\n",
              "author                                            xoshayzers  ...                                          xkilljoyx\n",
              "content    @tiffanylue i know  i was listenin to bad habi...  ...  @dannycastillo We want to trade with someone w...\n",
              "\n",
              "[4 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YpM_iet3Dub",
        "colab_type": "code",
        "outputId": "49fc5976-a6b6-4604-b9da-6bfc3c40d94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df.describe(include='all').T"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tweet_id</th>\n",
              "      <td>40000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.84518e+09</td>\n",
              "      <td>1.18858e+08</td>\n",
              "      <td>1.69396e+09</td>\n",
              "      <td>1.75143e+09</td>\n",
              "      <td>1.85544e+09</td>\n",
              "      <td>1.96278e+09</td>\n",
              "      <td>1.96644e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <td>40000</td>\n",
              "      <td>13</td>\n",
              "      <td>neutral</td>\n",
              "      <td>8638</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>author</th>\n",
              "      <td>40000</td>\n",
              "      <td>33871</td>\n",
              "      <td>MissxMarisa</td>\n",
              "      <td>23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>content</th>\n",
              "      <td>40000</td>\n",
              "      <td>39827</td>\n",
              "      <td>I just received a mothers day card from my lov...</td>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           count unique  ...          75%          max\n",
              "tweet_id   40000    NaN  ...  1.96278e+09  1.96644e+09\n",
              "sentiment  40000     13  ...          NaN          NaN\n",
              "author     40000  33871  ...          NaN          NaN\n",
              "content    40000  39827  ...          NaN          NaN\n",
              "\n",
              "[4 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8wNtJif3La-",
        "colab_type": "code",
        "outputId": "1f42c88b-a1d7-41a3-f067-a7d6882c1ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 4 columns):\n",
            "tweet_id     40000 non-null int64\n",
            "sentiment    40000 non-null object\n",
            "author       40000 non-null object\n",
            "content      40000 non-null object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 1.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "286MJ0KH0sXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[['content', 'sentiment']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89PAOTRLA9Oe",
        "colab_type": "code",
        "outputId": "fdbc2f9d-7ee4-4b8c-c2fd-700bacd1517a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "df.sentiment.value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral       8638\n",
              "worry         8459\n",
              "happiness     5209\n",
              "sadness       5165\n",
              "love          3842\n",
              "surprise      2187\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          827\n",
              "enthusiasm     759\n",
              "boredom        179\n",
              "anger          110\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl2PqJYha7Dn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "dbe95a95-b21f-4d87-b60c-74f350fd34b6"
      },
      "source": [
        "df[df.sentiment == 'neutral'][:10].content.values"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['@dannycastillo We want to trade with someone who has Houston tickets, but no one will.',\n",
              "       'cant fall asleep',\n",
              "       'No Topic Maps talks at the Balisage Markup Conference 2009   Program online at http://tr.im/mL6Z (via @bobdc) #topicmaps',\n",
              "       '@cynthia_123 i cant sleep', 'I missed the bl***y bus!!!!!!!!',\n",
              "       'feels strong contractions but wants to go out.  http://plurk.com/p/wxidk',\n",
              "       'SoCal!  stoked. or maybe not.. tomorrow',\n",
              "       'Screw you @davidbrussee! I only have 3 weeks...',\n",
              "       'has work this afternoon',\n",
              "       '@GABBYiSACTiVE Aw you would not unfollow me would you? Then I would cry'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2baQPREbJ3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keep_values(df, col_name, values):\n",
        "    return df.loc[df[col_name].isin(values), :].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LSu7UL5bUA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = keep_values(\n",
        "    df, \"sentiment\", df[\"sentiment\"].value_counts()[:5].index\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwZZH9QH5C50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# neutral      8638\n",
        "# worry        8459\n",
        "# happiness    5209\n",
        "# sadness      5165\n",
        "# love         3842\n",
        "# surprise     2187\n",
        "# fun          1776\n",
        "# relief       1526\n",
        "# hate         1323\n",
        "# Name: sentiment, dtype: int64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Kz4w4Obf9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "585f5a9b-4344-45fb-dc6b-076fca259571"
      },
      "source": [
        "df.sentiment.value_counts()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral      8638\n",
              "worry        8459\n",
              "happiness    5209\n",
              "sadness      5165\n",
              "love         3842\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fglZDbR01Ore",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_category_map(labels):\n",
        "    return {x: i for i, x in enumerate(sorted(set(labels)))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dBHa9Pq1dOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_category_map(filename, labels):\n",
        "    try:\n",
        "        category_map = load_file(filename)\n",
        "    except:\n",
        "        category_map = make_category_map(labels)\n",
        "        save_file(category_map, filename)\n",
        "    return category_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm52YHbe1jMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "67e19fd4-8f11-4bee-b030-1fe6910e10ae"
      },
      "source": [
        "category_map = init_category_map(\n",
        "    DATA_DIR / \"category_map.pickle\", df[\"sentiment\"].values\n",
        ")\n",
        "category_map"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'happiness': 0, 'love': 1, 'neutral': 2, 'sadness': 3, 'worry': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMiroXye2UrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "bbe7b67e-c42e-4e1d-8b64-520336879409"
      },
      "source": [
        "n_categories = len(category_map)\n",
        "n_categories"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRSTpaRb2XhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.sentiment = df.sentiment.map(category_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBZydIleytsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_container = DataContainer(\n",
        "    *DataBundle.split(\n",
        "        data_bundle=DataBundle.from_dataframe(\n",
        "            dataframe=df, x_col=\"content\", y_col=\"sentiment\"\n",
        "        ),\n",
        "        fracs=[0.8, 0.1, 0.1],\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSiNGJVOy8kI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "305b5aa3-97a9-4f44-8881-47065479e55a"
      },
      "source": [
        "len(data_container.train), len(data_container.validation), len(data_container.test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25051, 3132, 3130)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZKM-bnAcQe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = dict(\n",
        "    enumerate(\n",
        "        sk.utils.class_weight.compute_class_weight(\n",
        "            \"balanced\", np.unique(data_container.train.y), data_container.train.y\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7-V-aERcWoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "603774cf-9bd5-46db-de24-3679950b12f7"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.223790913531998,\n",
              " 1: 1.6357166176950702,\n",
              " 2: 0.7234945848375451,\n",
              " 3: 1.2006230529595014,\n",
              " 4: 0.7372277810476751}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq0zB2iGcbfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset_from_data_bundle(data_bundle):\n",
        "    return tf.data.Dataset.from_tensor_slices((data_bundle.x, data_bundle.y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLn6ov1RchuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_container.train.dataset = make_dataset_from_data_bundle(data_container.train)\n",
        "data_container.validation.dataset = make_dataset_from_data_bundle(\n",
        "    data_container.validation\n",
        ")\n",
        "data_container.test.dataset = make_dataset_from_data_bundle(data_container.test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGZ2_doOdHRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "6717a036-fce3-4287-daa3-fe72b7b9d185"
      },
      "source": [
        "data_container.train.x[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i am the only arabic girl who's online  every one is  a sleep ..\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY8AVetxdIri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "80e0edb5-fc13-48a9-c227-932a56d73471"
      },
      "source": [
        "data_container.train.y[0]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcQ3AKoVdMaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_sentence_length_limiter(limit):\n",
        "    def inner(x, y):\n",
        "        return tf.strings.substr(x, 0, limit), y\n",
        "\n",
        "    return inner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCJYotVcdYXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_limiter = make_sentence_length_limiter(500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL8480ivdorH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1B-MKAdeC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    data_container.train.dataset.repeat()\n",
        "    .shuffle(len(data_container.train))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(length_limiter)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nJxiTeNdqdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8b196ea0-1190-43e6-b260-b344ae614c6e"
      },
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    print(x[0])\n",
        "    print(y[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "(8,)\n",
            "tf.Tensor(b\"I never order chips any more due to how unhealthy they are, but getting a burrito from Chipotle or Qdoba doesn't feel right without em\", shape=(), dtype=string)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7U9fPINdu2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_dataset = (\n",
        "    data_container.validation.dataset.batch(BATCH_SIZE)\n",
        "    .map(length_limiter)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va8Atypmd1kw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7a9820f9-fa9d-47c1-c514-3b58cf36b5ea"
      },
      "source": [
        "for x, y in validation_dataset.take(1):\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    print(x[0])\n",
        "    print(y[0])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "(8,)\n",
            "tf.Tensor(b\"@BB517  not even a little bit biKnightual  (ha ha... tweedeck doesn't see the spelling problem with BK....ha!) but love it anyway.\", shape=(), dtype=string)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMiZSEHWd4xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_steps = math.ceil(len(data_container.train) / BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhVXJE9td9xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    epochs,\n",
        "    lrs=None,\n",
        "    optimizers=None,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    log_dir=str(DATA_DIR / \"logs\"),\n",
        "):\n",
        "    if optimizers is None:\n",
        "        optimizers = [keras.optimizers.Adam(lr) for lr in lrs]\n",
        "    model.layers[0].trainable = False\n",
        "    model.compile(\n",
        "        loss=keras.losses.sparse_categorical_crossentropy,\n",
        "        optimizer=optimizers[0],\n",
        "        metrics=[\n",
        "            keras.metrics.sparse_categorical_accuracy,\n",
        "            sparse_top_2_categorical_accuracy,\n",
        "        ],\n",
        "    )\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=train_dataset_steps,\n",
        "        validation_data=validation_dataset,\n",
        "        epochs=epochs[0],\n",
        "        callbacks=[\n",
        "            keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.3),\n",
        "            keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
        "        ],\n",
        "        class_weight=class_weights,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "    model.layers[0].trainable = True\n",
        "    model.compile(\n",
        "        loss=keras.losses.sparse_categorical_crossentropy,\n",
        "        optimizer=optimizers[1],\n",
        "        metrics=[\n",
        "            keras.metrics.sparse_categorical_accuracy,\n",
        "            sparse_top_2_categorical_accuracy,\n",
        "        ],\n",
        "    )\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=train_dataset_steps,\n",
        "        validation_data=validation_dataset,\n",
        "        epochs=epochs[1],\n",
        "        callbacks=[\n",
        "            keras.callbacks.ReduceLROnPlateau(patience=patience // 2, factor=0.3),\n",
        "            keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
        "            #             keras.callbacks.TensorBoard(log_dir=log_dir),\n",
        "        ],\n",
        "        class_weight=class_weights,\n",
        "        verbose=verbose,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynsu6kAjeDN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_hub_model(url):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            hub.KerasLayer(url, dtype=tf.string, input_shape=[]),\n",
        "            keras.layers.Dropout(0.5),\n",
        "            keras.layers.Dense(n_categories, activation=keras.activations.softmax),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q95RF8neG1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_models(urls):\n",
        "    for url in urls:\n",
        "        model = make_hub_model(url)\n",
        "        model_name = f\"{url.split('/')[4]}\"\n",
        "        print(f\" {model_name} \".center(80, \"=\"))\n",
        "        shutil.rmtree(str(DATA_DIR / model_name), ignore_errors=True)\n",
        "        train_model(\n",
        "            model=model,\n",
        "            epochs=[2, 3],\n",
        "            optimizers=[keras.optimizers.Adam(lr=1e-4), keras.optimizers.Adam(lr=3e-5)],\n",
        "            patience=2,\n",
        "            verbose=2,\n",
        "            log_dir=str(DATA_DIR / model_name),\n",
        "        )\n",
        "        model.save(f\"{DATA_DIR / model_name}.h5\")\n",
        "        save_keras_model(\n",
        "            model,\n",
        "            str(DATA_DIR / model_name / \"architecture\"),\n",
        "            str(DATA_DIR / model_name / \"weights\"),\n",
        "        )\n",
        "        keras.backend.clear_session()\n",
        "        del model\n",
        "        keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEDWaY9peW_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_urls = (\n",
        "    \"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\",\n",
        "    # \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\",\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6yJfaSgeX7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "7a0fb1c8-8e19-4ff4-a23e-f4ee4c0f38ee"
      },
      "source": [
        "# run_models(model_urls)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/Wiki-words-250-with-normalization/2'.\n",
            "INFO:absl:Downloading https://tfhub.dev/google/Wiki-words-250-with-normalization/2: 880.00MB\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/Wiki-words-250-with-normalization/2, Total size: 970.91MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/Wiki-words-250-with-normalization/2'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "Train for 3813 steps, validate for 477 steps\n",
            "Epoch 1/2\n",
            "3813/3813 - 17s - loss: 2.0476 - sparse_categorical_accuracy: 0.2331 - sparse_top_2_categorical_accuracy: 0.4200 - val_loss: 2.0016 - val_sparse_categorical_accuracy: 0.2523 - val_sparse_top_2_categorical_accuracy: 0.4616\n",
            "Epoch 2/2\n",
            "3813/3813 - 16s - loss: 2.0060 - sparse_categorical_accuracy: 0.2564 - sparse_top_2_categorical_accuracy: 0.4521 - val_loss: 1.9830 - val_sparse_categorical_accuracy: 0.2662 - val_sparse_top_2_categorical_accuracy: 0.4723\n",
            "Train for 3813 steps, validate for 477 steps\n",
            "Epoch 1/3\n",
            "3813/3813 - 260s - loss: 1.9789 - sparse_categorical_accuracy: 0.2780 - sparse_top_2_categorical_accuracy: 0.4753 - val_loss: 1.9534 - val_sparse_categorical_accuracy: 0.2927 - val_sparse_top_2_categorical_accuracy: 0.4980\n",
            "Epoch 2/3\n",
            "3813/3813 - 259s - loss: 1.9410 - sparse_categorical_accuracy: 0.2952 - sparse_top_2_categorical_accuracy: 0.5033 - val_loss: 1.9283 - val_sparse_categorical_accuracy: 0.3013 - val_sparse_top_2_categorical_accuracy: 0.5188\n",
            "Epoch 3/3\n",
            "3813/3813 - 261s - loss: 1.9131 - sparse_categorical_accuracy: 0.3209 - sparse_top_2_categorical_accuracy: 0.5270 - val_loss: 1.9059 - val_sparse_categorical_accuracy: 0.3142 - val_sparse_top_2_categorical_accuracy: 0.5269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2, Total size: 483.55MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====================== nnlm-en-dim128-with-normalization =======================\n",
            "Train for 3813 steps, validate for 477 steps\n",
            "Epoch 1/2\n",
            "3813/3813 - 15s - loss: 2.0754 - sparse_categorical_accuracy: 0.2250 - sparse_top_2_categorical_accuracy: 0.3987 - val_loss: 2.0067 - val_sparse_categorical_accuracy: 0.2609 - val_sparse_top_2_categorical_accuracy: 0.4626\n",
            "Epoch 2/2\n",
            "3813/3813 - 14s - loss: 2.0045 - sparse_categorical_accuracy: 0.2564 - sparse_top_2_categorical_accuracy: 0.4510 - val_loss: 1.9737 - val_sparse_categorical_accuracy: 0.2819 - val_sparse_top_2_categorical_accuracy: 0.4812\n",
            "Train for 3813 steps, validate for 477 steps\n",
            "Epoch 1/3\n",
            "3813/3813 - 138s - loss: 1.9773 - sparse_categorical_accuracy: 0.2751 - sparse_top_2_categorical_accuracy: 0.4742 - val_loss: 1.9516 - val_sparse_categorical_accuracy: 0.2911 - val_sparse_top_2_categorical_accuracy: 0.5001\n",
            "Epoch 2/3\n",
            "3813/3813 - 137s - loss: 1.9472 - sparse_categorical_accuracy: 0.2983 - sparse_top_2_categorical_accuracy: 0.4997 - val_loss: 1.9305 - val_sparse_categorical_accuracy: 0.3040 - val_sparse_top_2_categorical_accuracy: 0.5243\n",
            "Epoch 3/3\n",
            "3813/3813 - 134s - loss: 1.9213 - sparse_categorical_accuracy: 0.3130 - sparse_top_2_categorical_accuracy: 0.5195 - val_loss: 1.9108 - val_sparse_categorical_accuracy: 0.3207 - val_sparse_top_2_categorical_accuracy: 0.5460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epoNq8A1f3Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_models(urls, versions):\n",
        "    reports = {}\n",
        "    for url in urls:\n",
        "        for version in versions:\n",
        "            model_name = f\"{url.split('/')[4]}\"\n",
        "            print(f\" {model_name} \".center(80, \"=\"))\n",
        "            try:\n",
        "                model = keras.model.load_model(\n",
        "                    f\"{DATA_DIR / model_name}.h5\",\n",
        "                    custom_objects={\"KerasLayer\": hub.KerasLayer},\n",
        "                )\n",
        "            except:\n",
        "                print(f\"Loading architecture & weights separately\")\n",
        "                model = load_keras_model(\n",
        "                    str(DATA_DIR / model_name / \"architecture\"),\n",
        "                    str(DATA_DIR / model_name / \"weights\"),\n",
        "                    custom_objects={\"KerasLayer\": hub.KerasLayer},\n",
        "                )\n",
        "            reports[model_name] = classification_report(\n",
        "                data_container.validation.y,\n",
        "                model.predict(validation_dataset).argmax(axis=1),\n",
        "            )\n",
        "            del model\n",
        "    return reports"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pC0AK3vnWr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version_model_map = {\"base\": make_hub_model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FwH8UapgD6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "647e52a8-88ae-4435-c6ea-45f067bee884"
      },
      "source": [
        "# reports = evaluate_models(model_urls, version_model_map.keys())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "Loading architecture & weights separately\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====================== nnlm-en-dim128-with-normalization =======================\n",
            "Loading architecture & weights separately\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.Variable:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.Variable:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.Variable:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.Variable:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StD0BZo_rC9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "b496ed69-1035-4282-d41a-24c6fa643d03"
      },
      "source": [
        "# for model_name, report in reports.items():\n",
        "#     print(f\" {model_name} \".center(80, \"=\"))\n",
        "#     print(report)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       185\n",
            "           1       0.38      0.11      0.17       515\n",
            "           2       0.00      0.00      0.00       117\n",
            "           3       0.42      0.21      0.28       373\n",
            "           4       0.33      0.51      0.40       915\n",
            "           5       0.00      0.00      0.00       161\n",
            "           6       0.33      0.00      0.00       508\n",
            "           7       0.00      0.00      0.00       209\n",
            "           8       0.29      0.72      0.41       830\n",
            "\n",
            "    accuracy                           0.31      3813\n",
            "   macro avg       0.19      0.17      0.14      3813\n",
            "weighted avg       0.28      0.31      0.24      3813\n",
            "\n",
            "====================== nnlm-en-dim128-with-normalization =======================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       185\n",
            "           1       0.36      0.17      0.24       515\n",
            "           2       0.00      0.00      0.00       117\n",
            "           3       0.49      0.23      0.31       373\n",
            "           4       0.37      0.46      0.41       915\n",
            "           5       0.00      0.00      0.00       161\n",
            "           6       0.00      0.00      0.00       508\n",
            "           7       0.00      0.00      0.00       209\n",
            "           8       0.28      0.75      0.41       830\n",
            "\n",
            "    accuracy                           0.32      3813\n",
            "   macro avg       0.17      0.18      0.15      3813\n",
            "weighted avg       0.25      0.32      0.25      3813\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3izwF2r74Qhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ====================== Wiki-words-250-with-normalization =======================\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.00      0.00      0.00       185\n",
        "#            1       0.38      0.11      0.17       515\n",
        "#            2       0.00      0.00      0.00       117\n",
        "#            3       0.42      0.21      0.28       373\n",
        "#            4       0.33      0.51      0.40       915\n",
        "#            5       0.00      0.00      0.00       161\n",
        "#            6       0.33      0.00      0.00       508\n",
        "#            7       0.00      0.00      0.00       209\n",
        "#            8       0.29      0.72      0.41       830\n",
        "\n",
        "#     accuracy                           0.31      3813\n",
        "#    macro avg       0.19      0.17      0.14      3813\n",
        "# weighted avg       0.28      0.31      0.24      3813\n",
        "\n",
        "# ====================== nnlm-en-dim128-with-normalization =======================\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.00      0.00      0.00       185\n",
        "#            1       0.36      0.17      0.24       515\n",
        "#            2       0.00      0.00      0.00       117\n",
        "#            3       0.49      0.23      0.31       373\n",
        "#            4       0.37      0.46      0.41       915\n",
        "#            5       0.00      0.00      0.00       161\n",
        "#            6       0.00      0.00      0.00       508\n",
        "#            7       0.00      0.00      0.00       209\n",
        "#            8       0.28      0.75      0.41       830\n",
        "\n",
        "#     accuracy                           0.32      3813\n",
        "#    macro avg       0.17      0.18      0.15      3813\n",
        "# weighted avg       0.25      0.32      0.25      3813\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grz2_a27rKH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "441dab90-0b49-4415-9a44-28fe3c543e8d"
      },
      "source": [
        "data_container.train.value_counts()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1408,\n",
              " 1: 4125,\n",
              " 2: 1077,\n",
              " 3: 3105,\n",
              " 4: 6869,\n",
              " 5: 1215,\n",
              " 6: 4149,\n",
              " 7: 1790,\n",
              " 8: 6762}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubSaOF1kxFd4",
        "colab_type": "text"
      },
      "source": [
        "## Balance data for better accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK6MLmy4xELa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bundle = DataBundle.from_unbalanced(\n",
        "    data_container.train,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hlnu_WkwSgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "eccf3337-1af0-479d-afa6-e79571e7c0fa"
      },
      "source": [
        "train_bundle.value_counts()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 4094, 1: 6126, 2: 6925, 3: 4173, 4: 6796}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sRdABqk084_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_container.train.dataset = make_dataset_from_data_bundle(train_bundle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLFZwbSF1Gu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    data_container.train.dataset.repeat()\n",
        "    .shuffle(len(train_bundle))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(length_limiter)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vp8BNOo1Q_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "6a087f68-3be4-451a-c44c-dc90e6b3d475"
      },
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    print(x[0])\n",
        "    print(y[0])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8,)\n",
            "(8,)\n",
            "tf.Tensor(b'omg supernatural is on after good news week', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4WILAzz1URG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_steps = math.ceil(len(train_bundle) / BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVI_4ms1Z8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "eadb2059-f66e-475b-cb15-4b8d111165cb"
      },
      "source": [
        "train_dataset_steps"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEr-5a9SHpOA",
        "colab_type": "text"
      },
      "source": [
        "After epochs=[2, 5] resulsts was better, decided to train more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQKrV3Rx1tYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_models(urls, epochs):\n",
        "    for url in urls:\n",
        "        model = make_hub_model(url)\n",
        "        model_name = f\"{url.split('/')[4]}\"\n",
        "        print(f\" {model_name} \".center(80, \"=\"))\n",
        "        shutil.rmtree(str(DATA_DIR / model_name), ignore_errors=True)\n",
        "        train_model(\n",
        "            model=model,\n",
        "            epochs=epochs,\n",
        "            optimizers=[keras.optimizers.Adam(lr=1e-4), keras.optimizers.Adam(lr=3e-5)],\n",
        "            patience=2,\n",
        "            verbose=2,\n",
        "            log_dir=str(DATA_DIR / model_name),\n",
        "        )\n",
        "        model.save(f\"{DATA_DIR / model_name}.h5\")\n",
        "        save_keras_model(\n",
        "            model,\n",
        "            str(DATA_DIR / model_name / \"architecture\"),\n",
        "            str(DATA_DIR / model_name / \"weights\"),\n",
        "        )\n",
        "        keras.backend.clear_session()\n",
        "        del model\n",
        "        keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCMvz_Lp2K_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "d3de2571-f278-4381-faba-f7cf5d31e7eb"
      },
      "source": [
        "# run_models(model_urls)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "Train for 6457 steps, validate for 477 steps\n",
            "Epoch 1/2\n",
            "6457/6457 - 23s - loss: 2.1567 - sparse_categorical_accuracy: 0.1796 - sparse_top_2_categorical_accuracy: 0.3196 - val_loss: 2.1879 - val_sparse_categorical_accuracy: 0.1259 - val_sparse_top_2_categorical_accuracy: 0.2814\n",
            "Epoch 2/2\n",
            "6457/6457 - 22s - loss: 2.1309 - sparse_categorical_accuracy: 0.2019 - sparse_top_2_categorical_accuracy: 0.3505 - val_loss: 2.1536 - val_sparse_categorical_accuracy: 0.1883 - val_sparse_top_2_categorical_accuracy: 0.3622\n",
            "Train for 6457 steps, validate for 477 steps\n",
            "Epoch 1/5\n",
            "6457/6457 - 430s - loss: 2.0845 - sparse_categorical_accuracy: 0.2452 - sparse_top_2_categorical_accuracy: 0.3975 - val_loss: 2.1473 - val_sparse_categorical_accuracy: 0.1686 - val_sparse_top_2_categorical_accuracy: 0.3145\n",
            "Epoch 2/5\n",
            "6457/6457 - 429s - loss: 2.0273 - sparse_categorical_accuracy: 0.2810 - sparse_top_2_categorical_accuracy: 0.4358 - val_loss: 2.0899 - val_sparse_categorical_accuracy: 0.2046 - val_sparse_top_2_categorical_accuracy: 0.3724\n",
            "Epoch 3/5\n",
            "6457/6457 - 429s - loss: 1.9593 - sparse_categorical_accuracy: 0.3188 - sparse_top_2_categorical_accuracy: 0.4821 - val_loss: 2.0518 - val_sparse_categorical_accuracy: 0.2311 - val_sparse_top_2_categorical_accuracy: 0.4144\n",
            "Epoch 4/5\n",
            "6457/6457 - 429s - loss: 1.8983 - sparse_categorical_accuracy: 0.3460 - sparse_top_2_categorical_accuracy: 0.5188 - val_loss: 2.0190 - val_sparse_categorical_accuracy: 0.2578 - val_sparse_top_2_categorical_accuracy: 0.4346\n",
            "Epoch 5/5\n",
            "6457/6457 - 429s - loss: 1.8384 - sparse_categorical_accuracy: 0.3740 - sparse_top_2_categorical_accuracy: 0.5459 - val_loss: 1.9957 - val_sparse_categorical_accuracy: 0.2628 - val_sparse_top_2_categorical_accuracy: 0.4482\n",
            "====================== nnlm-en-dim128-with-normalization =======================\n",
            "Train for 6457 steps, validate for 477 steps\n",
            "Epoch 1/2\n",
            "6457/6457 - 22s - loss: 2.1600 - sparse_categorical_accuracy: 0.1767 - sparse_top_2_categorical_accuracy: 0.3138 - val_loss: 2.2102 - val_sparse_categorical_accuracy: 0.1469 - val_sparse_top_2_categorical_accuracy: 0.2486\n",
            "Epoch 2/2\n",
            "6457/6457 - 22s - loss: 2.1290 - sparse_categorical_accuracy: 0.2111 - sparse_top_2_categorical_accuracy: 0.3497 - val_loss: 2.1559 - val_sparse_categorical_accuracy: 0.1770 - val_sparse_top_2_categorical_accuracy: 0.3139\n",
            "Train for 6457 steps, validate for 477 steps\n",
            "Epoch 1/5\n",
            "6457/6457 - 224s - loss: 2.0749 - sparse_categorical_accuracy: 0.2552 - sparse_top_2_categorical_accuracy: 0.4025 - val_loss: 2.1470 - val_sparse_categorical_accuracy: 0.1665 - val_sparse_top_2_categorical_accuracy: 0.3068\n",
            "Epoch 2/5\n",
            "6457/6457 - 224s - loss: 2.0306 - sparse_categorical_accuracy: 0.2811 - sparse_top_2_categorical_accuracy: 0.4347 - val_loss: 2.0998 - val_sparse_categorical_accuracy: 0.1899 - val_sparse_top_2_categorical_accuracy: 0.3693\n",
            "Epoch 3/5\n",
            "6457/6457 - 224s - loss: 1.9773 - sparse_categorical_accuracy: 0.3040 - sparse_top_2_categorical_accuracy: 0.4711 - val_loss: 2.0613 - val_sparse_categorical_accuracy: 0.2187 - val_sparse_top_2_categorical_accuracy: 0.4060\n",
            "Epoch 4/5\n",
            "6457/6457 - 224s - loss: 1.9236 - sparse_categorical_accuracy: 0.3320 - sparse_top_2_categorical_accuracy: 0.5036 - val_loss: 2.0362 - val_sparse_categorical_accuracy: 0.2358 - val_sparse_top_2_categorical_accuracy: 0.4230\n",
            "Epoch 5/5\n",
            "6457/6457 - 224s - loss: 1.8706 - sparse_categorical_accuracy: 0.3565 - sparse_top_2_categorical_accuracy: 0.5330 - val_loss: 2.0148 - val_sparse_categorical_accuracy: 0.2444 - val_sparse_top_2_categorical_accuracy: 0.4346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD-PfNPh2PMO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "bd2e6b8c-06fe-445e-ce2f-31cbb4bd9d82"
      },
      "source": [
        "reports = evaluate_models(model_urls, version_model_map.keys())"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "Loading architecture & weights separately\n",
            "====================== nnlm-en-dim128-with-normalization =======================\n",
            "Loading architecture & weights separately\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Zrmk2u2uiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "9123b8b3-c205-4427-bacd-40f2a3ae626c"
      },
      "source": [
        "for model_name, report in reports.items():\n",
        "    print(f\" {model_name} \".center(80, \"=\"))\n",
        "    print(report)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.10      0.13       185\n",
            "           1       0.11      0.00      0.00       515\n",
            "           2       0.12      0.44      0.19       117\n",
            "           3       0.29      0.57      0.39       373\n",
            "           4       0.40      0.36      0.38       915\n",
            "           5       0.09      0.45      0.15       161\n",
            "           6       0.41      0.03      0.05       508\n",
            "           7       0.12      0.08      0.09       209\n",
            "           8       0.39      0.34      0.36       830\n",
            "\n",
            "    accuracy                           0.26      3813\n",
            "   macro avg       0.23      0.26      0.19      3813\n",
            "weighted avg       0.30      0.26      0.24      3813\n",
            "\n",
            "====================== nnlm-en-dim128-with-normalization =======================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      0.18      0.17       185\n",
            "           1       0.22      0.00      0.01       515\n",
            "           2       0.10      0.55      0.16       117\n",
            "           3       0.27      0.60      0.37       373\n",
            "           4       0.45      0.29      0.35       915\n",
            "           5       0.08      0.36      0.13       161\n",
            "           6       0.38      0.01      0.02       508\n",
            "           7       0.21      0.04      0.06       209\n",
            "           8       0.37      0.33      0.35       830\n",
            "\n",
            "    accuracy                           0.24      3813\n",
            "   macro avg       0.25      0.26      0.18      3813\n",
            "weighted avg       0.32      0.24      0.22      3813\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do2byGi052de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epochs=[2, 5]\n",
        "# ====================== Wiki-words-250-with-normalization =======================\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.17      0.10      0.13       185\n",
        "#            1       0.11      0.00      0.00       515\n",
        "#            2       0.12      0.44      0.19       117\n",
        "#            3       0.29      0.57      0.39       373\n",
        "#            4       0.40      0.36      0.38       915\n",
        "#            5       0.09      0.45      0.15       161\n",
        "#            6       0.41      0.03      0.05       508\n",
        "#            7       0.12      0.08      0.09       209\n",
        "#            8       0.39      0.34      0.36       830\n",
        "\n",
        "#     accuracy                           0.26      3813\n",
        "#    macro avg       0.23      0.26      0.19      3813\n",
        "# weighted avg       0.30      0.26      0.24      3813\n",
        "\n",
        "# ====================== nnlm-en-dim128-with-normalization =======================\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.16      0.18      0.17       185\n",
        "#            1       0.22      0.00      0.01       515\n",
        "#            2       0.10      0.55      0.16       117\n",
        "#            3       0.27      0.60      0.37       373\n",
        "#            4       0.45      0.29      0.35       915\n",
        "#            5       0.08      0.36      0.13       161\n",
        "#            6       0.38      0.01      0.02       508\n",
        "#            7       0.21      0.04      0.06       209\n",
        "#            8       0.37      0.33      0.35       830\n",
        "\n",
        "#     accuracy                           0.24      3813\n",
        "#    macro avg       0.25      0.26      0.18      3813\n",
        "# weighted avg       0.32      0.24      0.22      3813\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Kyf8KYhc2K",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piBgud1lImpX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "outputId": "45ded2e8-b32b-40a9-e2e7-7dd0c1813c65"
      },
      "source": [
        "run_models(model_urls, epochs=[2, 15])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/Wiki-words-250-with-normalization/2'.\n",
            "INFO:absl:Downloading https://tfhub.dev/google/Wiki-words-250-with-normalization/2: 790.00MB\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/Wiki-words-250-with-normalization/2, Total size: 970.91MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/Wiki-words-250-with-normalization/2'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "Train for 6434 steps, validate for 477 steps\n",
            "Epoch 1/2\n",
            "6434/6434 - 34s - loss: 2.1524 - sparse_categorical_accuracy: 0.1806 - sparse_top_2_categorical_accuracy: 0.3250 - val_loss: 2.1841 - val_sparse_categorical_accuracy: 0.1340 - val_sparse_top_2_categorical_accuracy: 0.2985\n",
            "Epoch 2/2\n",
            "6434/6434 - 32s - loss: 2.1314 - sparse_categorical_accuracy: 0.2017 - sparse_top_2_categorical_accuracy: 0.3510 - val_loss: 2.1454 - val_sparse_categorical_accuracy: 0.1886 - val_sparse_top_2_categorical_accuracy: 0.3669\n",
            "Train for 6434 steps, validate for 477 steps\n",
            "Epoch 1/15\n",
            "6434/6434 - 1290s - loss: 2.0850 - sparse_categorical_accuracy: 0.2430 - sparse_top_2_categorical_accuracy: 0.3954 - val_loss: 2.1423 - val_sparse_categorical_accuracy: 0.1778 - val_sparse_top_2_categorical_accuracy: 0.3331\n",
            "Epoch 2/15\n",
            "6434/6434 - 1292s - loss: 2.0234 - sparse_categorical_accuracy: 0.2807 - sparse_top_2_categorical_accuracy: 0.4400 - val_loss: 2.0771 - val_sparse_categorical_accuracy: 0.2001 - val_sparse_top_2_categorical_accuracy: 0.3884\n",
            "Epoch 3/15\n",
            "6434/6434 - 1291s - loss: 1.9610 - sparse_categorical_accuracy: 0.3143 - sparse_top_2_categorical_accuracy: 0.4816 - val_loss: 2.0321 - val_sparse_categorical_accuracy: 0.2305 - val_sparse_top_2_categorical_accuracy: 0.4212\n",
            "Epoch 4/15\n",
            "6434/6434 - 1292s - loss: 1.9011 - sparse_categorical_accuracy: 0.3446 - sparse_top_2_categorical_accuracy: 0.5122 - val_loss: 2.0122 - val_sparse_categorical_accuracy: 0.2402 - val_sparse_top_2_categorical_accuracy: 0.4351\n",
            "Epoch 5/15\n",
            "6434/6434 - 1292s - loss: 1.8474 - sparse_categorical_accuracy: 0.3670 - sparse_top_2_categorical_accuracy: 0.5428 - val_loss: 1.9879 - val_sparse_categorical_accuracy: 0.2478 - val_sparse_top_2_categorical_accuracy: 0.4482\n",
            "Epoch 6/15\n",
            "6434/6434 - 1296s - loss: 1.7908 - sparse_categorical_accuracy: 0.3924 - sparse_top_2_categorical_accuracy: 0.5712 - val_loss: 1.9607 - val_sparse_categorical_accuracy: 0.2654 - val_sparse_top_2_categorical_accuracy: 0.4650\n",
            "Epoch 7/15\n",
            "6434/6434 - 1293s - loss: 1.7390 - sparse_categorical_accuracy: 0.4130 - sparse_top_2_categorical_accuracy: 0.5965 - val_loss: 1.9477 - val_sparse_categorical_accuracy: 0.2680 - val_sparse_top_2_categorical_accuracy: 0.4705\n",
            "Epoch 8/15\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_2_categorical_accuracy,lr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_2_categorical_accuracy,lr\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_2_categorical_accuracy,lr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,sparse_top_2_categorical_accuracy,lr\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-090b9010bbfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-e688ff3af14e>\u001b[0m in \u001b[0;36mrun_models\u001b[0;34m(urls, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{DATA_DIR / model_name}.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-1353929c2af1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, lrs, optimizers, patience, verbose, log_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m         ],\n\u001b[1;32m     52\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m\"\"\"Provide a hash even if the input signature objects aren't hashable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields_safe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__get_cmp_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;34m\"\"\"Returns a hashable eq-comparable key for `self`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;31m# TODO(b/133606651): Decide whether to cache this value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__make_cmp_key\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;34m\"\"\"Converts `value` to a hashable key.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytes_or_text_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYtK_tTxhd9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "4f7c0003-ba35-4fa9-bbb0-23038493686e"
      },
      "source": [
        "reports = evaluate_models(model_urls, version_model_map.keys())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================== Wiki-words-250-with-normalization =======================\n",
            "Loading architecture & weights separately\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-49e781de6472>\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(urls, versions)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 model = keras.model.load_model(\n\u001b[0m\u001b[1;32m      9\u001b[0m                     \u001b[0;34mf\"{DATA_DIR / model_name}.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.keras' has no attribute 'model'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-91d0a82d4379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_model_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-49e781de6472>\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(urls, versions)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"architecture\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"KerasLayer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 )\n\u001b[1;32m     19\u001b[0m             reports[model_name] = classification_report(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/toai/models/load_model.py\u001b[0m in \u001b[0;36mload_keras_model\u001b[0;34m(architecture_path, weights_path, custom_objects)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m ):\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/emotions/Wiki-words-250-with-normalization/architecture'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whl7Dg--hfso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model_name, report in reports.items():\n",
        "    print(f\" {model_name} \".center(80, \"=\"))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me56fNKt7gw0",
        "colab_type": "text"
      },
      "source": [
        "## Use Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMC5fW-aHRYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "927b5c8e-3e35-43f0-e70f-83bd41338083"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 395599.89B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSDlthsz7i_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_pad(arr, tokenizer, sequence_length):\n",
        "    return np.array(\n",
        "        [\n",
        "            np.pad(\n",
        "                tokenizer.encode(x, add_special_tokens=True),\n",
        "                (0, sequence_length),\n",
        "                \"constant\",\n",
        "                constant_values=tokenizer.pad_token_id,\n",
        "            )[:sequence_length]\n",
        "            for x in arr\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJgOBQCI7pS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_container.train.x = tokenize_and_pad(train_bundle.x, tokenizer, 64)\n",
        "data_container.validation.x = tokenize_and_pad(data_container.validation.x, tokenizer, 64)\n",
        "data_container.test.x = tokenize_and_pad(data_container.test.x, tokenizer, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdmXqhNE8Cuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "32643a73-7fe4-4c4a-d66f-acbba68afc05"
      },
      "source": [
        "len(data_container.train.x)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAhkpCca8RPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_bert_dataset(data_bundle, tokenizer):\n",
        "    features = tf.data.Dataset.from_tensor_slices(data_bundle.x)\n",
        "    labels = tf.data.Dataset.from_tensor_slices(data_bundle.y)\n",
        "    dataset = tf.data.Dataset.zip((features, labels)).map(\n",
        "        lambda x, y: (\n",
        "            {\n",
        "                \"input_ids\": x,\n",
        "                \"attention_mask\": int(x != tokenizer.pad_token_id),\n",
        "                \"token_type_ids\": tf.zeros_like(x),\n",
        "            },\n",
        "            y,\n",
        "        )\n",
        "    )\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5mlpMe-8nfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_bert_dataset = make_bert_dataset(data_container.train, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAK6EoWh8uRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bert_dataset = (\n",
        "    base_bert_dataset.cache()\n",
        "    .repeat()\n",
        "    .shuffle(len(data_container.train.x))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcBJBha83bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_bert_dataset = (\n",
        "    make_bert_dataset(data_container.validation, tokenizer).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGTVp5N98-ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bert_dataset = (\n",
        "    make_bert_dataset(data_container.test, tokenizer).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUEPIA_M9lxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_label_map(filename, data_bundle):\n",
        "    try:\n",
        "        label_map = load_file(filename)\n",
        "    except:\n",
        "        label_map = data_bundle.make_label_map()\n",
        "        save_file(label_map, filename)\n",
        "    return label_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbhUemUm9oh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = init_label_map(DATA_DIR / \"label_map.pickle\", data_container.train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYmxbCj9-Mvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U toai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAA5C6gn-NkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a57340ca-15ba-4679-a663-a5f9f0ddca53"
      },
      "source": [
        "from toai.data import DataBundle, DataParams, DataContainer"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/toai/imports.py:70: UserWarning: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "  warnings.warn(str(error))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI1DENpx9EAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_container = DataContainer(\n",
        "    base=base_bert_dataset,\n",
        "    train=train_bert_dataset,\n",
        "    train_steps=len(data_container.train.x) // BATCH_SIZE,\n",
        "    validation=validation_bert_dataset,\n",
        "    test=test_bert_dataset,\n",
        "    label_map=label_map,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oD2i5Q39MQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "24e20260-f384-4bf1-8af4-fb3f5e7b084b"
      },
      "source": [
        "data_container.n_classes"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-UbxuNt-fIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    data_container,\n",
        "    epochs,\n",
        "    lrs=None,\n",
        "    optimizers=None,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    class_weights=None,\n",
        "    log_dir=str(DATA_DIR / \"logs\"),\n",
        "):\n",
        "    model.layers[0].trainable = False\n",
        "    if optimizers is None:\n",
        "        optimizers = [keras.optimizers.Adam(lr) for lr in lrs]\n",
        "    model.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=optimizers[0],\n",
        "        metrics=[\n",
        "            keras.metrics.sparse_categorical_accuracy,\n",
        "            sparse_top_2_categorical_accuracy,\n",
        "        ],\n",
        "    )\n",
        "    model.fit(\n",
        "        data_container.train,\n",
        "        steps_per_epoch=data_container.train_steps,\n",
        "        validation_data=data_container.validation,\n",
        "        epochs=epochs[0],\n",
        "        callbacks=[\n",
        "            keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.3),\n",
        "            keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
        "        ],\n",
        "        class_weight=class_weights,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "    model.layers[0].trainable = True\n",
        "    model.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=optimizers[1],\n",
        "        metrics=[\n",
        "            keras.metrics.sparse_categorical_accuracy,\n",
        "            sparse_top_2_categorical_accuracy,\n",
        "        ],\n",
        "    )\n",
        "    model.fit(\n",
        "        data_container.train,\n",
        "        steps_per_epoch=data_container.train_steps,\n",
        "        validation_data=data_container.validation,\n",
        "        epochs=epochs[1],\n",
        "        callbacks=[\n",
        "            keras.callbacks.ReduceLROnPlateau(patience=patience // 2, factor=0.3),\n",
        "            keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
        "            keras.callbacks.TensorBoard(log_dir=log_dir),\n",
        "        ],\n",
        "        class_weight=class_weights,\n",
        "        verbose=verbose,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPdRqI3R-mg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "12aa865c-180f-4645-ba4b-404e2264358d"
      },
      "source": [
        "config = transformers.BertConfig.from_pretrained(\n",
        "    \"bert-base-cased\", num_labels=data_container.n_classes\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 62156.96B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ZChvwN-uMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TFBertForSequenceClassification(transformers.TFBertPreTrainedModel):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(TFBertForSequenceClassification, self).__init__(config, *inputs, **kwargs)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = transformers.TFBertMainLayer(config, name=\"bert\")\n",
        "        self.dropout1 = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "        self.hidden1 = tf.keras.layers.Dense(1024, activation=tf.keras.activations.relu)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            config.num_labels,\n",
        "            kernel_initializer=transformers.modeling_tf_utils.get_initializer(\n",
        "                config.initializer_range\n",
        "            ),\n",
        "            name=\"classifier\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        outputs = self.bert(inputs, **kwargs)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout1(\n",
        "            pooled_output, training=kwargs.get(\"training\", False)\n",
        "        )\n",
        "        hidden = self.dropout2(\n",
        "            self.hidden1(pooled_output), training=kwargs.get(\"training\", False)\n",
        "        )\n",
        "        logits = self.classifier(hidden)\n",
        "\n",
        "        outputs = (logits,)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foKs-x1m-yz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "b12b24f2-701a-495e-d8d1-fcc5d7c50496"
      },
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\", config=config\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 526681800/526681800 [00:44<00:00, 11892963.36B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2NPuEls-1oM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "50f484e2-ae9e-4514-8826-331157be6dd8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  108310272 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  787456    \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  9225      \n",
            "=================================================================\n",
            "Total params: 109,106,953\n",
            "Trainable params: 109,106,953\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9IVRtF_GKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = dict(\n",
        "    enumerate(\n",
        "        sk.utils.class_weight.compute_class_weight(\n",
        "            \"balanced\", np.unique(train_bundle.y), train_bundle.y\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xl22Aac-4yD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "cd8b9599-8b97-4adc-b7ee-fa759fc5003d"
      },
      "source": [
        "train_model(\n",
        "    model,\n",
        "    data_container,\n",
        "    [0, 5],\n",
        "    [3e-6, 1e-6],\n",
        "    class_weights=class_weights,\n",
        "    patience=2,\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 6461 steps, validate for 477 steps\n",
            "Train for 6461 steps, validate for 477 steps\n",
            "Epoch 1/5\n",
            "6461/6461 [==============================] - 1791s 277ms/step - loss: 2.0071 - sparse_categorical_accuracy: 0.2256 - sparse_top_2_categorical_accuracy: 0.4463 - val_loss: 1.9918 - val_sparse_categorical_accuracy: 0.2533 - val_sparse_top_2_categorical_accuracy: 0.4472\n",
            "Epoch 2/5\n",
            "6461/6461 [==============================] - 1759s 272ms/step - loss: 2.0014 - sparse_categorical_accuracy: 0.2262 - sparse_top_2_categorical_accuracy: 0.4494 - val_loss: 1.9951 - val_sparse_categorical_accuracy: 0.2439 - val_sparse_top_2_categorical_accuracy: 0.4472\n",
            "Epoch 3/5\n",
            "6461/6461 [==============================] - 1757s 272ms/step - loss: 1.9998 - sparse_categorical_accuracy: 0.2312 - sparse_top_2_categorical_accuracy: 0.4460 - val_loss: 1.9942 - val_sparse_categorical_accuracy: 0.2347 - val_sparse_top_2_categorical_accuracy: 0.4472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKF--CPM_Uxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "57a4bf70-a2cd-41ae-8751-d20f7ee33599"
      },
      "source": [
        "print(\n",
        "    classification_report(\n",
        "        [label.numpy() for _, label in data_container.validation.unbatch()],\n",
        "        model.predict(data_container.validation).argmax(axis=1),\n",
        "    )\n",
        ")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       160\n",
            "           1       0.00      0.00      0.00       573\n",
            "           2       0.00      0.00      0.00       143\n",
            "           3       0.00      0.00      0.00       351\n",
            "           4       0.26      0.56      0.35       863\n",
            "           5       0.00      0.00      0.00       163\n",
            "           6       0.00      0.00      0.00       528\n",
            "           7       0.00      0.00      0.00       190\n",
            "           8       0.25      0.58      0.35       842\n",
            "\n",
            "    accuracy                           0.25      3813\n",
            "   macro avg       0.06      0.13      0.08      3813\n",
            "weighted avg       0.11      0.25      0.16      3813\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oTN0hzk_lw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHpXwHXziGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}